{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File:           Project_Model  \n",
    "  \n",
    "### Authors:        Brooke McWilliams, James Birch  \n",
    "  \n",
    "### Date Created:   11/19/2023  \n",
    "  \n",
    "### Last Modified:  12/04/2023  \n",
    "  \n",
    "### Description:    Strip features out of audio files using the librosa library and perform CNN modeling using the tensors and keras libraries  \n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load the WAV audio file data using librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    data, sampleRate = librosa.load(filepath)\n",
    "    return data, sampleRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign variables that will be used to store the data for modeling  \n",
    "Go through the Crema directory and append each files data to these variables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Crema Dataset/\"\n",
    "\n",
    "wav_data_mfcc_norm = []\n",
    "wav_data_rms_norm = []\n",
    "labelList = []\n",
    "wav_data = []\n",
    "fileName = []\n",
    "sampleR = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".wav\"):               \n",
    "        file_path = os.path.join(path, file)\n",
    "        # Get audio data from each data file\n",
    "        record, sr = get_data(file_path)\n",
    "        wav_data.append(record)\n",
    "        sampleR.append(sr)\n",
    "\n",
    "        # Labels for each data file\n",
    "        labelList.append(file.split('_')[2])\n",
    "\n",
    "        # File names list for tacking\n",
    "        fileName.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the max audio file length to use for padding so that the feature space is evenly distributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxL = 0\n",
    "minL = float('inf')\n",
    "for arr in wav_data:\n",
    "    length = len(arr)\n",
    "    if length > maxL:\n",
    "        maxL = length\n",
    "    if length < minL:\n",
    "        minL = length\n",
    "\n",
    "padded_data = []\n",
    "for signal in wav_data:\n",
    "    padW = maxL - len(signal)\n",
    "    padS = np.pad(signal, (0, padW), mode=\"constant\")\n",
    "    padded_data.append(padS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features from the read audio file data and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcwil\\AppData\\Local\\Temp\\ipykernel_8124\\3036714490.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  wav_data_rms_norm.append((wav_data_rms - np.mean(wav_data_rms)) / np.std(wav_data_rms))\n"
     ]
    }
   ],
   "source": [
    "for audio, rate in zip(padded_data, sampleR):\n",
    "    wav_data_mfcc = librosa.feature.mfcc(y=audio, sr=rate, n_mfcc=13).flatten()\n",
    "    wav_data_mfcc_norm.append((wav_data_mfcc - np.mean(wav_data_mfcc)) / np.std(wav_data_mfcc))\n",
    "\n",
    "    wav_data_rms = librosa.feature.rms(y=audio, hop_length=200).flatten()\n",
    "    wav_data_rms_norm.append((wav_data_rms - np.mean(wav_data_rms)) / np.std(wav_data_rms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframes to help work with the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCC\n",
    "wave_mfcc_dataframe = pd.DataFrame(wav_data_mfcc_norm)\n",
    "wave_mfcc_dataframe.insert(0, \"ID/File\", fileName)\n",
    "wave_mfcc_dataframe.insert(1, \"Labels\", labelList)\n",
    "\n",
    "#RMS\n",
    "wave_rms_dataframe = pd.DataFrame(wav_data_rms_norm)\n",
    "wave_rms_dataframe.insert(0, \"ID/File\", fileName)\n",
    "wave_rms_dataframe.insert(1, \"Labels\", labelList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframes to work with for each feature extraction method \n",
    "# MFCC\n",
    "wave_mfcc_dataframe = pd.DataFrame(wav_data_mfcc_norm)\n",
    "wave_mfcc_dataframe.insert(0, \"ID/File\", fileName)\n",
    "wave_mfcc_dataframe.insert(1, \"Labels\", labelList)\n",
    "\n",
    "#RMS\n",
    "wave_rms_dataframe = pd.DataFrame(wav_data_rms_norm)\n",
    "wave_rms_dataframe.insert(0, \"ID/File\", fileName)\n",
    "wave_rms_dataframe.insert(1, \"Labels\", labelList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Activation, BatchNormalization, Dropout, MaxPooling1D, Flatten, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the CNN model for training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train, y_train, X_test, y_test, num_label):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=256, kernel_size=5, padding='same', activation='relu', input_shape=(X_train.shape[1],1)))\n",
    "\n",
    "    model.add(Conv1D(filters=256, kernel_size=5, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same', strides=2))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same', strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same', strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_label, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing and encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X1_train: (5953, 2808)\n",
      "\n",
      "Shape of y_train: (5953, 6)\n",
      "\n",
      "Shape of X1_test: (1489, 2808)\n",
      "\n",
      "Shape of y_test: (1489, 6)\n",
      "\n",
      "Number of labels: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X1 = wave_mfcc_dataframe.iloc[:, 2:]\n",
    "y = wave_mfcc_dataframe['Labels']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "num_label = len(pd.unique(y))\n",
    "y = to_categorical(y, num_classes=6)\n",
    "\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X1_train: {X1_train.shape}\\n\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\\n\")\n",
    "print(f\"Shape of X1_test: {X1_test.shape}\\n\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\\n\")\n",
    "print(f\"Number of labels: {num_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send training data to model function and return compiled CNN model for MFCC  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_model(X1_train, y_train, X1_test, y_test, num_label)\n",
    "\n",
    "\n",
    "model1.fit(X1_train, y_train, epochs=50, batch_size=64, use_multiprocessing=True, validation_data=(X1_test, y_test))\n",
    "acc1 = model1.evaluate(X1_test, y_test)\n",
    "\n",
    "print(f\"Model Accuracy: {acc1[1]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X2_train: (5953, 552)\n",
      "\n",
      "Shape of y_train: (5953, 6)\n",
      "\n",
      "Shape of X2_test: (1489, 552)\n",
      "\n",
      "Shape of y_test: (1489, 6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X2 = wave_rms_dataframe.iloc[:, 2:]\n",
    "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X2_train: {X2_train.shape}\\n\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\\n\")\n",
    "print(f\"Shape of X2_test: {X2_test.shape}\\n\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send training data to model function and return compiled CNN model for RMS  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_190 (Conv1D)         (None, 552, 256)          1536      \n",
      "                                                                 \n",
      " conv1d_191 (Conv1D)         (None, 552, 256)          327936    \n",
      "                                                                 \n",
      " max_pooling1d_142 (MaxPool  (None, 276, 256)          0         \n",
      " ing1D)                                                          \n",
      "                                                                 \n",
      " conv1d_192 (Conv1D)         (None, 276, 128)          163968    \n",
      "                                                                 \n",
      " max_pooling1d_143 (MaxPool  (None, 138, 128)          0         \n",
      " ing1D)                                                          \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 138, 128)          0         \n",
      "                                                                 \n",
      " conv1d_193 (Conv1D)         (None, 138, 64)           41024     \n",
      "                                                                 \n",
      " max_pooling1d_144 (MaxPool  (None, 69, 64)            0         \n",
      " ing1D)                                                          \n",
      "                                                                 \n",
      " flatten_40 (Flatten)        (None, 4416)              0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                282688    \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 817542 (3.12 MB)\n",
      "Trainable params: 817542 (3.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "94/94 [==============================] - 27s 277ms/step - loss: 1.6958 - accuracy: 0.2641 - val_loss: nan - val_accuracy: 0.3680\n",
      "Epoch 2/100\n",
      "94/94 [==============================] - 26s 278ms/step - loss: 1.5513 - accuracy: 0.3536 - val_loss: nan - val_accuracy: 0.3969\n",
      "Epoch 3/100\n",
      "94/94 [==============================] - 26s 277ms/step - loss: 1.5092 - accuracy: 0.3775 - val_loss: nan - val_accuracy: 0.3936\n",
      "Epoch 4/100\n",
      "94/94 [==============================] - 29s 307ms/step - loss: 1.5055 - accuracy: 0.3810 - val_loss: nan - val_accuracy: 0.4077\n",
      "Epoch 5/100\n",
      "94/94 [==============================] - 27s 282ms/step - loss: 1.4824 - accuracy: 0.4050 - val_loss: nan - val_accuracy: 0.4056\n",
      "Epoch 6/100\n",
      "94/94 [==============================] - 27s 283ms/step - loss: 1.4640 - accuracy: 0.3968 - val_loss: nan - val_accuracy: 0.4291\n",
      "Epoch 7/100\n",
      "94/94 [==============================] - 25s 269ms/step - loss: 1.4460 - accuracy: 0.4141 - val_loss: nan - val_accuracy: 0.4271\n",
      "Epoch 8/100\n",
      "94/94 [==============================] - 26s 276ms/step - loss: 1.4628 - accuracy: 0.4077 - val_loss: nan - val_accuracy: 0.4117\n",
      "Epoch 9/100\n",
      "94/94 [==============================] - 28s 297ms/step - loss: 1.4275 - accuracy: 0.4233 - val_loss: nan - val_accuracy: 0.4157\n",
      "Epoch 10/100\n",
      "94/94 [==============================] - 27s 288ms/step - loss: 1.4234 - accuracy: 0.4198 - val_loss: nan - val_accuracy: 0.4325\n",
      "Epoch 11/100\n",
      "94/94 [==============================] - 26s 279ms/step - loss: 1.4009 - accuracy: 0.4347 - val_loss: nan - val_accuracy: 0.4332\n",
      "Epoch 12/100\n",
      "94/94 [==============================] - 26s 273ms/step - loss: 1.4025 - accuracy: 0.4361 - val_loss: nan - val_accuracy: 0.4379\n",
      "Epoch 13/100\n",
      "93/94 [============================>.] - ETA: 0s - loss: 1.4212 - accuracy: 0.4219"
     ]
    }
   ],
   "source": [
    "model2 = build_model(X2_train, y_train, X2_test, y_test, num_label)\n",
    "\n",
    "model2.fit(X2_train, y_train, epochs=100, batch_size=64, validation_data=(X2_test, y_test))\n",
    "acc2 = model2.evaluate(X2_test, y_test)\n",
    "\n",
    "print(f\"Model Accuracy: {acc2[1]:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
